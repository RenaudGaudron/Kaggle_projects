{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Importing the relevant Python libraries and setting up the environment\n",
    "\n",
    "%reset -f\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "np.set_printoptions(threshold = 1e6)\n",
    "\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# function to count number of parameters\n",
    "def get_n_params(model):\n",
    "    np=0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.nelement()\n",
    "    return np\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing the data\n",
    "\n",
    "data_train=pd.read_csv('../Data/train.csv')\n",
    "data_test=pd.read_csv('../Data/test.csv')\n",
    "\n",
    "# Defining dataframes\n",
    "\n",
    "df_train=pd.DataFrame(data_train)\n",
    "df_test=pd.DataFrame(data_test)\n",
    "\n",
    "# # Validation - Checking random images from the training and testing sets\n",
    "# plt.imshow(df_test.loc[0].to_numpy().reshape(1, 28, 28).squeeze())\n",
    "# plt.imshow(df_train.drop(columns='label').loc[0].to_numpy().reshape(1, 28, 28).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Y_train: (33600,)\n",
      "Shape of X_train: (33600, 784)\n",
      "Shape of Y_val: (8400,)\n",
      "Shape of X_val: (8400, 784)\n",
      "Shape of X_test: (28000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Defining the input matrices and output vectors\n",
    "\n",
    "X=df_train.drop(columns='label').values # Inputs for the training and validation set\n",
    "Y=df_train['label'].values  # Labels for the training and validation set\n",
    "X_test=df_test.values # Inputs for the testing set\n",
    "\n",
    "# # Validation - Checking some images\n",
    "# plt.imshow(X_test[0,:].reshape(1, 28, 28).squeeze())\n",
    "\n",
    "# Normalising the features\n",
    "scaler = preprocessing.StandardScaler(with_mean=True, with_std=True).fit(X) # Defining the scaler\n",
    "X=scaler.transform(X) # Applying the scaler to the training and validation set\n",
    "X_test=scaler.transform(X_test) # Applying the scaler to the testing set\n",
    "\n",
    "# # Validation - Checking some images\n",
    "# plt.imshow(X_test[0,:].reshape(1, 28, 28).squeeze())\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2) # Splitting into a training and validation set\n",
    "\n",
    "# Checking the shape of the resulting matrices/vectors\n",
    "print('Shape of Y_train:', Y_train.shape)\n",
    "print('Shape of X_train:', X_train.shape)\n",
    "print('Shape of Y_val:', Y_val.shape)\n",
    "print('Shape of X_val:', X_val.shape)\n",
    "print('Shape of X_test:',X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Dataset Class\n",
    "\n",
    "class Digit_train(Dataset): # Class for the training and cross-validation set\n",
    "    def __init__(self, X1, Y1):\n",
    "        self.X = X1 # Contains the image \n",
    "        self.Y = Y1 # Contains the label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X[:,0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx, :]\n",
    "        image = image.reshape(1, 28, 28)\n",
    "        label = self.Y[idx]\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "class Digit_test(Dataset): # Class for the testing set\n",
    "    def __init__(self, X1):\n",
    "        self.X = X1 # Contains the image \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X[:,0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx, :]\n",
    "        image = image.reshape(1, 28, 28)\n",
    "        \n",
    "        return image\n",
    "    \n",
    "# Creating the datasets\n",
    "\n",
    "train_dataset = Digit_train(X_train, Y_train)\n",
    "val_dataset = Digit_train(X_val,Y_val)\n",
    "test_dataset = Digit_test(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the dataloaders\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining the class corresponding to the Convolutional Neural Net (CNN)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, n_feature, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=n_feature, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(n_feature, n_feature, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(n_feature*4*4, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.dropout = nn.Dropout2d(0.1)\n",
    "        \n",
    "    def forward(self, x, verbose=False):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = x.view(-1, self.n_feature*4*4)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 333260\n",
      "Epoch: 1/100 -- Training loss: 0.478 -- Training Accuracy: 86.637% -- Validation Accuracy: 95.583%\n",
      "Epoch: 2/100 -- Training loss: 0.139 -- Training Accuracy: 95.875% -- Validation Accuracy: 97.131%\n",
      "Epoch: 3/100 -- Training loss: 0.094 -- Training Accuracy: 97.250% -- Validation Accuracy: 97.571%\n",
      "Epoch: 4/100 -- Training loss: 0.075 -- Training Accuracy: 97.661% -- Validation Accuracy: 98.048%\n",
      "Epoch: 5/100 -- Training loss: 0.065 -- Training Accuracy: 97.964% -- Validation Accuracy: 98.214%\n",
      "Epoch: 6/100 -- Training loss: 0.056 -- Training Accuracy: 98.292% -- Validation Accuracy: 98.298%\n",
      "Epoch: 7/100 -- Training loss: 0.047 -- Training Accuracy: 98.560% -- Validation Accuracy: 98.333%\n",
      "Epoch: 8/100 -- Training loss: 0.043 -- Training Accuracy: 98.619% -- Validation Accuracy: 98.500%\n",
      "Epoch: 9/100 -- Training loss: 0.040 -- Training Accuracy: 98.768% -- Validation Accuracy: 98.524%\n",
      "Epoch: 10/100 -- Training loss: 0.035 -- Training Accuracy: 98.949% -- Validation Accuracy: 98.690%\n",
      "Epoch: 11/100 -- Training loss: 0.031 -- Training Accuracy: 99.018% -- Validation Accuracy: 98.548%\n",
      "Epoch: 12/100 -- Training loss: 0.029 -- Training Accuracy: 99.092% -- Validation Accuracy: 98.690%\n",
      "Epoch: 13/100 -- Training loss: 0.028 -- Training Accuracy: 99.083% -- Validation Accuracy: 98.679%\n",
      "Epoch: 14/100 -- Training loss: 0.025 -- Training Accuracy: 99.202% -- Validation Accuracy: 98.857%\n",
      "Epoch: 15/100 -- Training loss: 0.023 -- Training Accuracy: 99.268% -- Validation Accuracy: 98.750%\n",
      "Epoch: 16/100 -- Training loss: 0.021 -- Training Accuracy: 99.336% -- Validation Accuracy: 98.845%\n",
      "Epoch: 17/100 -- Training loss: 0.021 -- Training Accuracy: 99.348% -- Validation Accuracy: 98.857%\n",
      "Epoch: 18/100 -- Training loss: 0.018 -- Training Accuracy: 99.446% -- Validation Accuracy: 98.750%\n",
      "Epoch: 19/100 -- Training loss: 0.018 -- Training Accuracy: 99.443% -- Validation Accuracy: 98.702%\n",
      "Epoch: 20/100 -- Training loss: 0.016 -- Training Accuracy: 99.488% -- Validation Accuracy: 98.821%\n",
      "Epoch: 21/100 -- Training loss: 0.016 -- Training Accuracy: 99.536% -- Validation Accuracy: 98.857%\n",
      "Epoch: 22/100 -- Training loss: 0.014 -- Training Accuracy: 99.562% -- Validation Accuracy: 98.881%\n",
      "Epoch: 23/100 -- Training loss: 0.015 -- Training Accuracy: 99.524% -- Validation Accuracy: 98.893%\n",
      "Epoch: 24/100 -- Training loss: 0.012 -- Training Accuracy: 99.664% -- Validation Accuracy: 98.893%\n",
      "Epoch: 25/100 -- Training loss: 0.012 -- Training Accuracy: 99.640% -- Validation Accuracy: 98.917%\n",
      "Epoch: 26/100 -- Training loss: 0.011 -- Training Accuracy: 99.658% -- Validation Accuracy: 98.821%\n",
      "Epoch: 27/100 -- Training loss: 0.010 -- Training Accuracy: 99.643% -- Validation Accuracy: 98.964%\n",
      "Epoch: 28/100 -- Training loss: 0.010 -- Training Accuracy: 99.696% -- Validation Accuracy: 98.952%\n",
      "Epoch: 29/100 -- Training loss: 0.009 -- Training Accuracy: 99.723% -- Validation Accuracy: 98.893%\n",
      "Epoch: 30/100 -- Training loss: 0.010 -- Training Accuracy: 99.717% -- Validation Accuracy: 98.833%\n",
      "Epoch: 31/100 -- Training loss: 0.009 -- Training Accuracy: 99.720% -- Validation Accuracy: 98.976%\n",
      "Epoch: 32/100 -- Training loss: 0.008 -- Training Accuracy: 99.815% -- Validation Accuracy: 98.952%\n",
      "Epoch: 33/100 -- Training loss: 0.008 -- Training Accuracy: 99.768% -- Validation Accuracy: 98.893%\n",
      "Epoch: 34/100 -- Training loss: 0.007 -- Training Accuracy: 99.774% -- Validation Accuracy: 98.988%\n",
      "Epoch: 35/100 -- Training loss: 0.007 -- Training Accuracy: 99.815% -- Validation Accuracy: 99.012%\n",
      "Epoch: 36/100 -- Training loss: 0.007 -- Training Accuracy: 99.789% -- Validation Accuracy: 98.869%\n",
      "Epoch: 37/100 -- Training loss: 0.006 -- Training Accuracy: 99.818% -- Validation Accuracy: 98.881%\n",
      "Epoch: 38/100 -- Training loss: 0.006 -- Training Accuracy: 99.818% -- Validation Accuracy: 98.964%\n",
      "Epoch: 39/100 -- Training loss: 0.007 -- Training Accuracy: 99.798% -- Validation Accuracy: 99.036%\n",
      "Epoch: 40/100 -- Training loss: 0.006 -- Training Accuracy: 99.869% -- Validation Accuracy: 98.988%\n",
      "Epoch: 41/100 -- Training loss: 0.005 -- Training Accuracy: 99.890% -- Validation Accuracy: 98.917%\n",
      "Epoch: 42/100 -- Training loss: 0.005 -- Training Accuracy: 99.836% -- Validation Accuracy: 98.988%\n",
      "Epoch: 43/100 -- Training loss: 0.004 -- Training Accuracy: 99.884% -- Validation Accuracy: 98.893%\n",
      "Epoch: 44/100 -- Training loss: 0.005 -- Training Accuracy: 99.860% -- Validation Accuracy: 98.821%\n",
      "Epoch: 45/100 -- Training loss: 0.004 -- Training Accuracy: 99.920% -- Validation Accuracy: 99.048%\n",
      "Epoch: 46/100 -- Training loss: 0.004 -- Training Accuracy: 99.887% -- Validation Accuracy: 99.000%\n",
      "Epoch: 47/100 -- Training loss: 0.004 -- Training Accuracy: 99.872% -- Validation Accuracy: 99.036%\n",
      "Epoch: 48/100 -- Training loss: 0.004 -- Training Accuracy: 99.887% -- Validation Accuracy: 99.000%\n",
      "Epoch: 49/100 -- Training loss: 0.004 -- Training Accuracy: 99.884% -- Validation Accuracy: 99.024%\n",
      "Epoch: 50/100 -- Training loss: 0.004 -- Training Accuracy: 99.908% -- Validation Accuracy: 98.893%\n",
      "Epoch: 51/100 -- Training loss: 0.004 -- Training Accuracy: 99.878% -- Validation Accuracy: 98.893%\n",
      "Epoch: 52/100 -- Training loss: 0.003 -- Training Accuracy: 99.905% -- Validation Accuracy: 99.000%\n",
      "Epoch: 53/100 -- Training loss: 0.004 -- Training Accuracy: 99.920% -- Validation Accuracy: 99.060%\n",
      "Epoch: 54/100 -- Training loss: 0.003 -- Training Accuracy: 99.914% -- Validation Accuracy: 99.036%\n",
      "Epoch: 55/100 -- Training loss: 0.004 -- Training Accuracy: 99.887% -- Validation Accuracy: 99.024%\n",
      "Epoch: 56/100 -- Training loss: 0.003 -- Training Accuracy: 99.908% -- Validation Accuracy: 98.929%\n",
      "Epoch: 57/100 -- Training loss: 0.003 -- Training Accuracy: 99.917% -- Validation Accuracy: 99.012%\n",
      "Epoch: 58/100 -- Training loss: 0.003 -- Training Accuracy: 99.943% -- Validation Accuracy: 99.000%\n",
      "Epoch: 59/100 -- Training loss: 0.003 -- Training Accuracy: 99.893% -- Validation Accuracy: 98.881%\n",
      "Epoch: 60/100 -- Training loss: 0.003 -- Training Accuracy: 99.929% -- Validation Accuracy: 98.988%\n",
      "Epoch: 61/100 -- Training loss: 0.003 -- Training Accuracy: 99.929% -- Validation Accuracy: 99.000%\n",
      "Epoch: 62/100 -- Training loss: 0.003 -- Training Accuracy: 99.917% -- Validation Accuracy: 99.060%\n",
      "Epoch: 63/100 -- Training loss: 0.003 -- Training Accuracy: 99.932% -- Validation Accuracy: 98.917%\n",
      "Epoch: 64/100 -- Training loss: 0.003 -- Training Accuracy: 99.920% -- Validation Accuracy: 99.131%\n",
      "Epoch: 65/100 -- Training loss: 0.003 -- Training Accuracy: 99.923% -- Validation Accuracy: 98.905%\n",
      "Epoch: 66/100 -- Training loss: 0.002 -- Training Accuracy: 99.958% -- Validation Accuracy: 99.060%\n",
      "Epoch: 67/100 -- Training loss: 0.002 -- Training Accuracy: 99.958% -- Validation Accuracy: 99.048%\n",
      "Epoch: 68/100 -- Training loss: 0.002 -- Training Accuracy: 99.938% -- Validation Accuracy: 99.012%\n",
      "Epoch: 69/100 -- Training loss: 0.002 -- Training Accuracy: 99.938% -- Validation Accuracy: 99.071%\n",
      "Epoch: 70/100 -- Training loss: 0.003 -- Training Accuracy: 99.940% -- Validation Accuracy: 99.000%\n",
      "Epoch: 71/100 -- Training loss: 0.002 -- Training Accuracy: 99.967% -- Validation Accuracy: 99.107%\n",
      "Epoch: 72/100 -- Training loss: 0.002 -- Training Accuracy: 99.964% -- Validation Accuracy: 99.071%\n",
      "Epoch: 73/100 -- Training loss: 0.002 -- Training Accuracy: 99.952% -- Validation Accuracy: 99.095%\n",
      "Epoch: 74/100 -- Training loss: 0.003 -- Training Accuracy: 99.929% -- Validation Accuracy: 98.952%\n",
      "Epoch: 75/100 -- Training loss: 0.002 -- Training Accuracy: 99.940% -- Validation Accuracy: 99.060%\n",
      "Epoch: 76/100 -- Training loss: 0.002 -- Training Accuracy: 99.946% -- Validation Accuracy: 98.988%\n",
      "Epoch: 77/100 -- Training loss: 0.003 -- Training Accuracy: 99.920% -- Validation Accuracy: 99.048%\n",
      "Epoch: 78/100 -- Training loss: 0.002 -- Training Accuracy: 99.958% -- Validation Accuracy: 99.012%\n",
      "Epoch: 79/100 -- Training loss: 0.002 -- Training Accuracy: 99.938% -- Validation Accuracy: 99.095%\n",
      "Epoch: 80/100 -- Training loss: 0.002 -- Training Accuracy: 99.943% -- Validation Accuracy: 99.048%\n",
      "Epoch: 81/100 -- Training loss: 0.002 -- Training Accuracy: 99.967% -- Validation Accuracy: 99.083%\n",
      "Epoch: 82/100 -- Training loss: 0.001 -- Training Accuracy: 99.982% -- Validation Accuracy: 99.119%\n",
      "Epoch: 83/100 -- Training loss: 0.002 -- Training Accuracy: 99.946% -- Validation Accuracy: 99.131%\n",
      "Epoch: 84/100 -- Training loss: 0.002 -- Training Accuracy: 99.940% -- Validation Accuracy: 99.119%\n",
      "Epoch: 85/100 -- Training loss: 0.002 -- Training Accuracy: 99.943% -- Validation Accuracy: 99.107%\n",
      "Epoch: 86/100 -- Training loss: 0.002 -- Training Accuracy: 99.949% -- Validation Accuracy: 99.083%\n",
      "Epoch: 87/100 -- Training loss: 0.001 -- Training Accuracy: 99.979% -- Validation Accuracy: 99.024%\n",
      "Epoch: 88/100 -- Training loss: 0.001 -- Training Accuracy: 99.979% -- Validation Accuracy: 99.024%\n",
      "Epoch: 89/100 -- Training loss: 0.002 -- Training Accuracy: 99.961% -- Validation Accuracy: 99.024%\n",
      "Epoch: 90/100 -- Training loss: 0.002 -- Training Accuracy: 99.970% -- Validation Accuracy: 99.083%\n",
      "Epoch: 91/100 -- Training loss: 0.002 -- Training Accuracy: 99.940% -- Validation Accuracy: 99.012%\n",
      "Epoch: 92/100 -- Training loss: 0.002 -- Training Accuracy: 99.958% -- Validation Accuracy: 99.024%\n",
      "Epoch: 93/100 -- Training loss: 0.002 -- Training Accuracy: 99.949% -- Validation Accuracy: 99.048%\n",
      "Epoch: 94/100 -- Training loss: 0.001 -- Training Accuracy: 99.970% -- Validation Accuracy: 98.988%\n",
      "Epoch: 95/100 -- Training loss: 0.001 -- Training Accuracy: 99.976% -- Validation Accuracy: 99.060%\n",
      "Epoch: 96/100 -- Training loss: 0.001 -- Training Accuracy: 99.967% -- Validation Accuracy: 99.000%\n",
      "Epoch: 97/100 -- Training loss: 0.001 -- Training Accuracy: 99.979% -- Validation Accuracy: 99.071%\n",
      "Epoch: 98/100 -- Training loss: 0.001 -- Training Accuracy: 99.979% -- Validation Accuracy: 98.976%\n",
      "Epoch: 99/100 -- Training loss: 0.001 -- Training Accuracy: 99.970% -- Validation Accuracy: 99.060%\n",
      "Epoch: 100/100 -- Training loss: 0.001 -- Training Accuracy: 99.973% -- Validation Accuracy: 98.964%\n"
     ]
    }
   ],
   "source": [
    "# Training the network\n",
    "\n",
    "input_size  = X.shape[1]   # number of pixels in the image\n",
    "output_size = 10      # there are 10 classes\n",
    "n_features = 100 # number of feature maps\n",
    "\n",
    "model_cnn = CNN(input_size, n_features, output_size) # Instantiating the network\n",
    "model_cnn.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_cnn)))\n",
    "\n",
    "model_cnn = model_cnn.double()\n",
    "\n",
    "n_epochs = 100 # Number of times the entire dataset is used to train the network\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Training loop \n",
    "    model_cnn.train() # Setting the model to train mode\n",
    "    train_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        # clear the old gradients from optimized variables\n",
    "        optimizer.zero_grad() \n",
    "        # forward pass: feed inputs to the model to get outputs\n",
    "        output = model_cnn(data)\n",
    "        # calculate the training batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward: perform gradient descent of the loss w.r. to the model params\n",
    "        loss.backward()\n",
    "        # update the model parameters by performing a single optimization step\n",
    "        optimizer.step()\n",
    "        # accumulate the training loss\n",
    "        train_loss += loss.item()\n",
    "        # calculate the accuracy\n",
    "        predicted_train = torch.argmax(output, 1)\n",
    "        correct_train += (predicted_train == target).sum().item()\n",
    "        total_train += target.size(0)\n",
    "\n",
    "    # Validation loop \n",
    "    model_cnn.eval() # Setting the model to eval mode\n",
    "    val_loss = 0\n",
    "    # turn off gradients for validation\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(val_loader):\n",
    "            # forward pass\n",
    "            output = model_cnn(data)\n",
    "            # validation batch loss\n",
    "            loss = criterion(output, target) \n",
    "            # accumulate the valid_loss\n",
    "            val_loss += loss.item()\n",
    "            # calculate the accuracy\n",
    "            predicted_val = torch.argmax(output, 1)\n",
    "            correct_val += (predicted_val == target).sum().item()\n",
    "            total_val += target.size(0)\n",
    "\n",
    "    # Printing epoch results        \n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Epoch: {epoch+1}/{n_epochs} -- Training loss: {train_loss:.3f} -- Training Accuracy: {100*correct_train/total_train:.3f}% -- Validation Accuracy: {100*correct_val/total_val:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the output and writing results to a file\n",
    "\n",
    "result=[] # Used to store the results\n",
    "ind=[]  # Used to store the indices\n",
    "\n",
    "model_cnn.eval() # setting the model to eval mode\n",
    "# turn off gradients for validation\n",
    "with torch.no_grad():\n",
    "    for index, data_test in enumerate(test_loader):\n",
    "        output_test = model_cnn(data_test.to(device))\n",
    "        # save the results\n",
    "        predicted_test = torch.argmax(output_test, 1)\n",
    "        ind.append(index+1)\n",
    "        result.append(predicted_test.item())\n",
    "\n",
    "final_data=np.column_stack((ind, result)) # Stacking the two columns\n",
    "np.savetxt(\"submission.csv\", final_data, delimiter=\",\", header=\"ImageId,Label\", fmt='%d,%d', comments='') # Saving to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
